{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "practical1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Zavi0t3vaBvN"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "Xv2OlMsg2Bpc"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/HSinger04/DL4NLP/blob/master/src/assignment1.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ighIhhiLaBtR"
      },
      "source": [
        "## Practical Exercise 1: word2vec\n",
        "By Joline Janz and Frederik Wollatz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG7euCYV2Bpj"
      },
      "source": [
        "Each Notebook will contribute equaly to your final grade."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCmr1p64aBtW"
      },
      "source": [
        "This practical Exercise is presented as an IPython Notebook, with the code written for recent versions of **Python 3**. \n",
        "\n",
        "To execute a notebook cell, press `shift-enter`. The return value of the last command will be displayed, if it is not `None`.\n",
        "\n",
        "Potentially useful library documentation, references, and resources:\n",
        "\n",
        "* IPython notebooks: <https://ipython.org/ipython-doc/3/notebook/notebook.html#introduction>\n",
        "* Numpy numerical array library: <https://docs.scipy.org/doc/>\n",
        "* Gensim's word2vec: <https://radimrehurek.com/gensim/models/word2vec.html>\n",
        "* Bokeh interactive plots: <http://bokeh.pydata.org/en/latest/> (we provide plotting code here, but click the thumbnails for more examples to copy-paste)\n",
        "* scikit-learn ML library (aka `sklearn`): <http://scikit-learn.org/stable/documentation.html>\n",
        "* nltk NLP toolkit: <http://www.nltk.org/>\n",
        "* tutorial for processing xml in python using `lxml`: <http://lxml.de/tutorial.html> (we did this for you below, but in case you need it in the future)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoKd2JJZ2Bpo"
      },
      "source": [
        "In this Notebook you will learn the basics on how to construct a word-embedding. As you recall from the lecture, word-embeddings are a type of word representation that allows words with similar meaning to have a similar representation. To do this, words are represented as real-valued vectors in a predefined vector space. Additionally, you will also learn how to use some basic NLP tools like tokenization and regular Expressions!\n",
        "Good Luck!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "569W6MImaBtX"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from random import shuffle\n",
        "import re"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsgimgrUaBtd"
      },
      "source": [
        "from bokeh.models import ColumnDataSource, LabelSet\n",
        "from bokeh.plotting import figure, show, output_file\n",
        "from bokeh.io import output_notebook\n",
        "output_notebook()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofHeuGejaZHs"
      },
      "source": [
        "try: \n",
        "    import nltk\n",
        "except:\n",
        "    import sys #Here we install nltk. You only have to execute this cell once!\n",
        "    !{sys.executable} -m pip install nltk \n",
        "    import nltk\n",
        "    nltk.download()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4ibbJSg3MUu",
        "outputId": "0a1be1bc-ff46-4844-8396-5f7c280083e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# if not installed yet\n",
        "!pip3 install num2words"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: num2words in /usr/local/lib/python3.6/dist-packages (0.5.10)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.6/dist-packages (from num2words) (0.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Haav4K7YaBti"
      },
      "source": [
        "### Part 0: Download the TED dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IWfXKbC2Bpw"
      },
      "source": [
        "As input we need a large amount of text data. We will use the TED database, which are the transcripts of Ted Talks. The next cells will download everything you need, this might take a while as the dataset is 75MB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezhHjPQK2GZy",
        "outputId": "c930ef8b-ef96-48ea-abf5-d44f5659863f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Only for Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "# set this to \"\" if not using Google Drive\n",
        "gdrive_path = \"/content/gdrive/My Drive/DL4NLP/\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stf464BBaBtj"
      },
      "source": [
        "import urllib.request\n",
        "import zipfile\n",
        "import lxml.etree"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91T_rkPbaBtn"
      },
      "source": [
        "# Download the dataset if it's not already there: this may take a minute as it is 75MB\n",
        "if not (os.path.isfile('ted_en-20160408.zip') or gdrive_path):\n",
        "    urllib.request.urlretrieve(\"https://wit3.fbk.eu/get.php?path=XML_releases/xml/ted_en-20160408.zip&filename=ted_en-20160408.zip\", filename=\"ted_en-20160408.zip\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oWf1JkvaBtr"
      },
      "source": [
        "# For now, we're only interested in the subtitle text, so let's extract that from the XML:\n",
        "with zipfile.ZipFile(gdrive_path + 'ted_en-20160408.zip', 'r') as z:\n",
        "    doc = lxml.etree.parse(z.open('ted_en-20160408.xml', 'r'))\n",
        "input_text = '\\n'.join(doc.xpath('//content/text()'))\n",
        "del doc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9UVfpExaBtu"
      },
      "source": [
        "### Part 1: Preprocessing\n",
        "\n",
        "Before using our text, we need to preprocess it. Therefore, we bring it into a form that is predictable and analyzable. We attempt to clean up the raw subtitles a bit, so that we get only complete sentences. The following substring shows examples of what we're trying to get rid of. Since it's hard to define precisely what we want to get rid of, we'll just use some simple heuristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKWHxHG7bkWa"
      },
      "source": [
        "<h4>Execercise 1.1 (2 Points)</h4> \n",
        "Before we work with the data we should have a look at it. We already marked some areas for you, that need to be cleaned. You do not have to code anything here, you just have to become aware of sensitive preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGpx6RFzaBtv",
        "scrolled": false,
        "outputId": "cbd36a42-fdb4-4e84-92fc-3db446fc9982"
      },
      "source": [
        "#Have a look at the output of this code, to see some examples\n",
        "i = input_text.find(\"Hyowon Gweon: See this?\")\n",
        "print(input_text[i:i+145])\n",
        "\n",
        "\n",
        "i = input_text.find(\"You will earn\")\n",
        "print(input_text[i:i+30])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hyowon Gweon: See this? (Ball squeaks) Did you see that? (Ball squeaks) Cool. See this one? (Ball squeaks) Wow.\n",
            "Laura Schulz: Told you. (Laughs)\n",
            "\n",
            "You will earn 10% of any gold \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y7gtjkWcJ-2"
      },
      "source": [
        "For example the parenthesized strings like \"(Ball squeaks)\" and symbols like % could distort the semantics of words in the embedding. Name at least two more problematic sections and how you would solve them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRn3imW72Bp9"
      },
      "source": [
        "<b>Your Solution:</b> \n",
        "<br>- Parenthesized Strings\n",
        "<br>- Percent-Symbol\n",
        "<br>- Speaker Identifier (e.g. \"Hyowon Gweon:\")\n",
        "<br>- Number-Symbol\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH2seZr2aBt1"
      },
      "source": [
        "<h4>Exercise 1.2 (2 Points)</h4>\n",
        "Let's start by removing all parenthesized strings using a regex:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u3YXkh5aBt1",
        "outputId": "d378487f-fdc0-4b3c-b727-346b082b29e9"
      },
      "source": [
        "i = input_text.find(\"Hyowon Gweon: See this?\")\n",
        "print(\"before\")\n",
        "print(input_text[i:i+93])\n",
        "\n",
        "input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text) #Identifies everything in parenthesis and replaces it with \"\"\n",
        "\n",
        "\n",
        "#you can use this to verify\n",
        "i = input_text_noparens.find(\"Hyowon Gweon: See this?\")\n",
        "print(\"after\")\n",
        "print(input_text_noparens[i:i+93])\n",
        "\n",
        "#We won't worry about the irregular spaces since we'll later split the text into sentences and tokenize it anyway."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before\n",
            "Hyowon Gweon: See this? (Ball squeaks) Did you see that? (Ball squeaks) Cool. See this one? (\n",
            "after\n",
            "Hyowon Gweon: See this?  Did you see that?  Cool. See this one?  Wow.\n",
            "Laura Schulz: Told you.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtwPNpChdjww"
      },
      "source": [
        "Try it yourself: Replace every percentage Symbol with the word \"percent\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o6oEW0AdiDY",
        "outputId": "49687f58-edff-4dd3-9515-012bfda7375b"
      },
      "source": [
        "i = input_text_noparens.find(\"You will earn\")\n",
        "print(\"before\")\n",
        "print(input_text_noparens[i:i+30])\n",
        "\n",
        "input_text_clean = re.sub(\"%\", \" percent\", input_text_noparens)\n",
        "\n",
        "i = input_text_clean.find(\"You will earn\")\n",
        "print(\"after\")\n",
        "print(input_text_clean[i:i+37])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before\n",
            "You will earn 10% of any gold \n",
            "after\n",
            "You will earn 10 percent of any gold \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67ZOyc_9e2aS"
      },
      "source": [
        "<h4>Exercise 1.3 (up to 6 Points)</h4>\n",
        "Now you have learned how to use RegEx to your advantage and have Identified potential parts of the text, that we want to eliminate. We have already implented how to remove all parenthesized strings. It is now your Task to implement at least one more heuristic to replace the problematic parts you previously identified(Exercise 1.1).\n",
        "A simple replacing function (like the one where you replaced \"%\" with \"percent\") gives you two points per function, more complicated implementations will give you the full 6 Points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6NKj6fNfXMQ",
        "pycharm": {
          "is_executing": true
        },
        "outputId": "a97c87a7-a975-480a-8ef3-a1be0b6e3cc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from num2words import num2words\n",
        "\n",
        "search_str = \"by does.\"\n",
        "\n",
        "i = input_text_clean.find(search_str) #find problematic parts\n",
        "print(\"before\")\n",
        "print(input_text_clean[i:i+37]) #and show them\n",
        "\n",
        "# Remove speaker identifiers. Unlike in 1.4, we only remove\n",
        "# substrings before ':' that either consist of two capital letters\n",
        "# for initials or is two words starting with capital letters.\n",
        "# Thus, the problem from 1.4 is resolved, though this ofc\n",
        "# also isn't a perfect solution (e.g. the speaker identifier \"Ray:\"\n",
        "# does not get detected with this method.\n",
        "new_input_text_clean = re.sub(\n",
        "    r'([A-Z]{2}|[A-Z][a-z]+\\s[A-Z][a-z]+):', '', input_text_clean)\n",
        "\n",
        "i = new_input_text_clean.find(search_str) #validate your method\n",
        "print(\"after\")\n",
        "print(new_input_text_clean[i:i+37])\n",
        "\n",
        "\n",
        "search_str = \"You will earn\"\n",
        "i = new_input_text_clean.find(search_str)\n",
        "print(\"before\")\n",
        "print(new_input_text_clean[i:i+30])\n",
        "\n",
        "# TODO\n",
        "def num_to_word(line):\n",
        "    \"\"\"\n",
        "    finds and replaces FIRST number in numeric format with\n",
        "    word representation.\n",
        "\n",
        "    :param line:\n",
        "    :return: if digit to replace was found, slightly cleaned line\n",
        "    \"\"\"\n",
        "\n",
        "    # check in line for all maximal number occurences\n",
        "    numbers = re.findall(r'[0-9]+', line)\n",
        "    # if number in numeric format exists, replace\n",
        "    for num in numbers:\n",
        "        # TODO\n",
        "        line = re.sub(\n",
        "            # TODO\n",
        "            num,\n",
        "            num2words(num),\n",
        "            new_input_text_clean\n",
        "        )\n",
        "    return line\n",
        "\n",
        "X = []\n",
        "for line in new_input_text_clean.split('\\n'):\n",
        "\n",
        "    line = num_to_word(line)\n",
        "    # store in X\n",
        "    X.append(line)\n",
        "\n",
        "new_input_text_clean=\"\".join(X)\n",
        "\n",
        "\"\"\"\n",
        "numbers = re.findall(, new_input_text_clean)\n",
        "for number in numbers:\n",
        "    new_input_text_clean = re.sub(number, num2words(number), new_input_text_clean)\n",
        "\"\"\"\n",
        "\n",
        "search_str = \"You will earn\"\n",
        "i = new_input_text_clean.find(search_str)\n",
        "print(\"after\")\n",
        "print(new_input_text_clean[i:i+30])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before\n",
            "by does.\n",
            " Hyowon Gweon: See this?  Di\n",
            "after\n",
            "by does.\n",
            "  See this?  Did you see tha\n",
            "before\n",
            "You will earn 10 percent of an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf2I0FH99VSX"
      },
      "source": [
        "## TODO: Deleted a cell accidentally"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iESrOqQ32BqQ"
      },
      "source": [
        "i = input_text_clean.find(\"Hyowon Gweon: See this?\")\n",
        "print(\"before:\")\n",
        "print(input_text_clean[i-31:i+92])\n",
        "\n",
        "X = []\n",
        "for line in input_text_clean.split('\\n'):\n",
        "    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
        "    X.extend(m.groupdict()['postcolon'])\n",
        "input_text_clean2=\"\".join(X)\n",
        "\n",
        "\n",
        "i = input_text_clean2.find(\"See this?\")\n",
        "print(\"after:\")\n",
        "print(input_text_clean2[i-31:i+55])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "XtKO1-v82BqQ"
      },
      "source": [
        "This code block removes the speaker identifiers (e.g. Laura Schulz).\n",
        "The flaw is that it removes anything at the start of a new line\n",
        "before the first \":\". However, this also detects and thus deletes\n",
        "strings like 'Second one:' which appear at the start, but are no speaker identifiers\n",
        "(These can be found by running re.findall(r'\\n[^:]{,20}:', new_input_text_clean))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX5lFA4XijPm",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "<h4>Exercise 1.5 (6 Points)</h4>\n",
        "\n",
        "To build our embedding we need to tokenize every single word. Therefore we first need to split the text into sentences and after that into words. \n",
        "Try it yourself or use the NLTK-Tools build for this (https://www.kite.com/python/docs/nltk.word_tokenize + https://www.kite.com/python/docs/nltk.sent_tokenize).\n",
        "To make it easier to build our Embedding we should also delete every character that is not a letter. Additionally, we could lower vocabulary count. A way to do this is by converting capital characters to lower case characters.\n",
        "\n",
        "Split your text into sentences and save them in the array `sentences_strings_ted`.\n",
        "Save one variabale `tokens` with all the tokens in the text and one array named `sentences_ted` that contains an array for every sentence, with all the tokenized words of that sentence.<br><br>\n",
        "Example:<br>\n",
        "If the Text looks like this: \"I love cake. You have to be honest, you love it too!\", the variables look like:<br><br>\n",
        "sentences_strings_ted=['I love cake.', 'You have to be honest, you love it too!']<br>\n",
        "sentences_ted=[['i', 'love', 'cake'], ['you', 'have', 'to', 'be', 'honest', 'you', 'love', 'it', 'too']]<br>\n",
        "tokens=['i', 'love', 'cake', 'you', 'have', 'to', 'be', 'honest', 'you', 'love', 'it', 'too']<br>\n",
        "\n",
        "\n",
        "Apply this to `new_input_text_clean`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "SNcfLdHg2BqR",
        "outputId": "2d1d33a0-469e-4319-ff74-356e4fe93901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "sentences_strings_ted = None\n",
        "\n",
        "try:\n",
        "    sentences_strings_ted = nltk.sent_tokenize(\n",
        "        new_input_text_clean\n",
        "    )\n",
        "except:\n",
        "    nltk.download(\"punkt\")\n",
        "    sentences_strings_ted = nltk.sent_tokenize(\n",
        "        new_input_text_clean\n",
        "    )\n",
        "\n",
        "# only keeps tokens that consist of lowercase alphabet letters only\n",
        "reg_tok = RegexpTokenizer(r'[a-z]+')\n",
        "\n",
        "sentences_ted = \\\n",
        "    [reg_tok.tokenize(sent.lower())\n",
        "     for sent in sentences_strings_ted]\n",
        "\n",
        "\n",
        "tokens = \\\n",
        "    [token for sentence in sentences_ted for token in sentence]\n",
        "\n",
        "# print results\n",
        "print(sentences_strings_ted[:2])\n",
        "print(sentences_ted[:2])\n",
        "print(tokens[:50])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ca24fec9b91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     sentences_strings_ted = nltk.sent_tokenize(\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnew_input_text_clean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 834\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n    - ''\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-ca24fec9b91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"punkt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msentences_strings_ted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_input_text_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# only keeps tokens that consist of lowercase alphabet letters only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \"\"\"\n\u001b[1;32m   1336\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1471\u001b[0m         \"\"\"\n\u001b[1;32m   1472\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1473\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_second_pass_annotation\u001b[0;34m(self, aug_tok1, aug_tok2)\u001b[0m\n\u001b[1;32m   1488\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1490\u001b[0;31m         \u001b[0mtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_tok1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_no_period\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1491\u001b[0m         \u001b[0mnext_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_tok2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1492\u001b[0m         \u001b[0mnext_typ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maug_tok2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_no_sentperiod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtype_no_period\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mits\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0mperiod\u001b[0m \u001b[0mremoved\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \"\"\"\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUiFC0I5j31i"
      },
      "source": [
        "<h4>Exercise 1.6 (1 Point)</h4>\n",
        "The good side is, that by converting all capital letters is, we reduce the volume of the vocabulary. Thereby we dont differentiate between the the words \"today\" and \"Today\". \n",
        "Can you think of any downside to this process?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw04JKzB2BqT"
      },
      "source": [
        "Proper nouns are characterized by their capital letters. If e.g. we would have an entity called 'Blue', but also the color 'blue' in our text, we wouldn't be abe to distinguish between them anymore even though they are different words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Fs3pzZmHHv"
      },
      "source": [
        "Now we can have a look at the processed dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooe1gfSZaBuG"
      },
      "source": [
        "len(sentences_ted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwdw8QxbaBuJ"
      },
      "source": [
        "print(sentences_ted[0])\n",
        "print(sentences_ted[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCjkAGHjaBuN"
      },
      "source": [
        "### Part 2: Word Frequencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvLMH0iXaBuO"
      },
      "source": [
        "<h4>Exercise 2.1 (4 Points)</h4>\n",
        "Your next task will be to store the counts of the top 1000 most frequent words in a list called `counts_ted_top1000` ! There are multiple ways to do this. You can have a look at the Counter-Function(https://docs.python.org/2/library/collections.html) or the FreqDist-Function (https://www.kite.com/python/docs/nltk.FreqDist). If you dont trust any of those you can of course build your own function.\n",
        "In the end we want an array with tupels of the structure [(WordA,FrequencyA),(WordB,FrequencyB)]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcQzY8iWaBuP"
      },
      "source": [
        "counts_ted_top1000 = nltk.FreqDist(tokens)\n",
        "\n",
        "counts_ted_top1000tupels = [(word, counts_ted_top1000[word]) for word in counts_ted_top1000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hti5NFdGaBuX"
      },
      "source": [
        "The following code is going to plot a histogramm of the distribution of the  top-30 words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKpWmQz12Bqd"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "mostfreqn=30 #Here we define how many of them we want to see in the diagramm \n",
        "frequency=[y for (x,y) in counts_ted_top1000tupels][:mostfreqn]\n",
        "word=[x for (x,y) in counts_ted_top1000tupels][:mostfreqn]\n",
        "indices = np.arange(len(counts_ted_top1000tupels[:mostfreqn]))\n",
        "plt.bar(indices, frequency, color='r')\n",
        "plt.xticks(indices, word, rotation='vertical')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf6gXJU32Bqd"
      },
      "source": [
        "You can clearly see, that many of the most common words are so called stop words. Stop Words are words, that are tipically not usefull to identify what a text is about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oRdD3e4aBud"
      },
      "source": [
        "### Part 3: Train Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp0JGmKWhrNd"
      },
      "source": [
        "Now it is time to train the modell. Gensim has an already implemented model that you can use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTA9yWtdhb6z"
      },
      "source": [
        "Using the provided modell is enough for the purposes of our notebook. If you want to dive deeper into the topic this youtube video https://www.youtube.com/watch?v=kKDYtZfriI8 could be a great guidance for you to get started. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_t4-aiTaBue"
      },
      "source": [
        "#This takes a moment...dont worry :D\n",
        "from gensim.models import Word2Vec\n",
        "model_ted = Word2Vec(sentences_ted)\n",
        "\n",
        "word_vectors = model_ted.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKYd7ZemaBuj"
      },
      "source": [
        "### Part 4: Ted Learnt Representations (3 Points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7VvU82AaBuj"
      },
      "source": [
        "Finding similar words: (see gensim docs for functions, that might help you https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agPp1iR2Bqh"
      },
      "source": [
        "Now lets explore what we can do with this! How does \"house\" look in our embedding?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-TJQUI12Bqi"
      },
      "source": [
        "word_vectors['house']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW4_WKIf2Bqj"
      },
      "source": [
        "What is the most similar word for \"town\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0F5B8eN2Bqj"
      },
      "source": [
        "word_vectors.most_similar(positive=\"town\", topn=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTrzOibc2Bqm"
      },
      "source": [
        "How similar are the words \"town\" and \"house\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Evb9ktsZ2Bqn"
      },
      "source": [
        "word_vectors.cosine_similarities(word_vectors['house'], [word_vectors['town']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7I5G-KpqbfH"
      },
      "source": [
        "<h4>Exercise 4.1 (3 Points)</h4>\n",
        "Now that we have trained our own embedding, lets test some classical ideas: \n",
        "implement the following formula. Print out the 10 words, that are most similar to this formula: <br>\n",
        "$King-Man+Woman=???$\n",
        "There are two ways of computing similarity in word Embeddings:\n",
        " - https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html\n",
        " - https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar_cosmul.html\n",
        "You should try out both! In this case one of them is better, but both of them are valid methods for computing similarity in the word-space.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0y_5MLlqiMx"
      },
      "source": [
        "print(word_vectors.most_similar(\n",
        "    positive=[\"king\", \"woman\"], negative=[\"man\"]))\n",
        "print(word_vectors.most_similar_cosmul(\n",
        "    positive=[\"king\", \"woman\"], negative=[\"man\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSsUPsxo2Bqp"
      },
      "source": [
        "The expected outcome (Queen) should be one of the top ten most similar words. But there are also a lot of words, that you would not expect. Think about where how these words might be connected to the formula. You do not have to write anything down for this task, just take your time and understand why some of the words (luther, mary, dr, president) might be in this list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMtpPOVtaBup"
      },
      "source": [
        "#### t-SNE visualization\n",
        "\n",
        "We will use the t-SNE algorithm, given belwo, for visualization. The so-called t-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised and non-linear machine learning technique. It is commonly used for visualizing high dimensional data (just like our high dimensional vectors). You do not have to understand the code, it's purpose is simply to give you an idea of how the data is arranged in high dimensional space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnuijI2s2Bqq"
      },
      "source": [
        "<h4>Exercise 4.2 (2 Points)</h4>\n",
        "To use the t-SNE code below, first put a list of the top 100 words (as strings) into a variable `words_top_ted`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2gYgJ2q2Bqr"
      },
      "source": [
        "words_top_ted = list(counts_ted_top1000)[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SYLalLf2Bqr"
      },
      "source": [
        "The following code gets the corresponding vectors from the model, assuming it's called `model_ted`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-lLF1lZaBus"
      },
      "source": [
        "# This assumes words_top_ted is a list of strings, the top 250 words\n",
        "words_top_vec_ted = model_ted[words_top_ted]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvRU-2bT2Bqs"
      },
      "source": [
        "The next few lines are for the t-SNE visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeJF5ut9aBux"
      },
      "source": [
        "from sklearn.manifold import TSNE \n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "words_top_ted_tsne = tsne.fit_transform(words_top_vec_ted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2VgYLIZaBu2"
      },
      "source": [
        "p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
        "           toolbar_location=\"above\",\n",
        "           title=\"word2vec T-SNE for most common words\")\n",
        "\n",
        "source = ColumnDataSource(data=dict(x1=words_top_ted_tsne[:,0],\n",
        "                                    x2=words_top_ted_tsne[:,1],\n",
        "                                    names=words_top_ted))\n",
        "\n",
        "p.scatter(x=\"x1\", y=\"x2\", size=8, source=source)\n",
        "\n",
        "labels = LabelSet(x=\"x1\", y=\"x2\", text=\"names\", y_offset=6,\n",
        "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
        "                  source=source, text_align='center')\n",
        "p.add_layout(labels)\n",
        "\n",
        "show(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbvq-xfh4A5Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}